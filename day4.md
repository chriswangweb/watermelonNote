决策树：
根节点，内部节点，叶子节点

树的选择：
信息增益
增益率
基尼指数

解决过拟合问题：剪枝处理，预剪枝，后剪枝

数据=信息+噪音
信息：
1.消除不确定性
2.调整概率
3.排除干扰
4.确定情况
----------------------------------
熵的量化：
1.均匀分布
2.一般分布，各种情况概率不同

信息的量化

import math
print ("小明不知道选择题ABCD哪个选项时的熵 : ", math.log(4,2))
print ("告诉小明C有一半几率是正确提供的信息 : ", math.log(4,2)-(1/6*math.log(6,2)+1/6*math.log(6,2)+1/2*math.log(2,2)+1/6*math.log(6,2)))
print ("小明知道C有一半几率是正确后的熵 : ", 1/6*math.log(6,2)+1/6*math.log(6,2)+1/2*math.log(2,2)+1/6*math.log(6,2))

小明不知道选择题ABCD哪个选项时的熵 :  2.0
告诉小明C有一半几率是正确提供的信息 :  0.20751874963942218
小明知道C有一半几率是正确后的熵 :  1.7924812503605778

补充知识：
1.为什么计算熵时对数底数是 2。因为熵的单位是 bit
2.一般分布的计算公式  p*log(1/p,2)求和，p 是出现的概率

信息增益：对各个因素计算信息量（信息增益），信息量大的优先作为分类条件；分类后重复之前步骤即针对各个分类计算其他因素的信息增益
然而 信息增益对可取值数目较多的属性（因素）有所偏好，为了解决这个问题，出现了增益率，具体实现需要研究论文，大概意思是除以一个数字，这个数字随着属性可取值数目增多会增大，进行抑制

具体使用方式：
第一步使用信息增益得到排名靠前的属性
第二步针对得到的属性，计算增益率，选择增益率高的属性

基尼指数
有放回的取两个样本，两个不同的概率

具体使用方法：
第一步根据各种属性分别做二分类
第二步计算各个二分类的基尼指数
第三步取基尼指数最小的属性作为根分类
第四步在之前分类的基础上计算各个分类基尼指数，如果下降继续分类，如果上升停止分类
第五步如果某分类下样本数量小于预定义的值（根据个人情况设置），则停止分类



